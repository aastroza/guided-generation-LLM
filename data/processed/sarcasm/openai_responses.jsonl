{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "dozens of white houses materialize from temporal vortex as trump's changing account of putin meeting tears apart space-time"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.8e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00059}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000124}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00131}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000133}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00138}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "13 essential questions to ask when hiring a web design company"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "police use exact right amount of force to subdue suspect"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000115}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0012}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "senators target the 'many-headed dragon' of climate change denial"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00121}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "the president wore a tan suit, broke the internet"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00046}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000112}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00119}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00012}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "nude model suspects she's posing for civics class"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000114}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00119}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "u.n. warns trump may be 7 months away from acquiring nuclear weapons"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005099999999999999}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011899999999999999}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000125}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013000000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "'trump' chant used to intimidate latino high school athletes"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00011399999999999999}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "company to use internet to waste money, employees' time"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000113}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0012}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "powerball ticket sold with all winning numbers in $421 million jackpot"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "dog not sure it ready to tackle whatever happened to man at work today"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "area dad botches 'princess bride' quote"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00046}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000112}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00119}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00012}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "jennifer garner makes first public appearance since ben affleck split"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "an eye-opening look at school playgrounds around the world"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000115}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "even regular exercise isn't enough to cancel out too much sitting"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "fetus going to pretend he doesn't hear loud argument coming from other side of uterine wall"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00056}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00128}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00013000000000000002}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00135}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "friends of band regret going to show"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00043000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011100000000000001}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00116}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000117}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "'oh, was i not enough for you?' amazon echo asks couple bringing new baby home"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.300000000000001e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00054}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00012}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00127}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000128}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013300000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "parents honor beloved dead grandmother by naming baby 'gamgam'"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00011399999999999999}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00121}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "things come apart so easily: asghar farhadi's about elly"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.1000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00052}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011999999999999999}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000126}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013100000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "the end of shared sacrifice set in stone: yale as metaphor"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "what to expect when you're expecting a couch"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.4000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00045}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011300000000000001}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012100000000000001}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000119}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00124}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "to wax or not to wax?"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00043000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011100000000000001}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00119}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "cannon overshoots tim kaine across wells fargo center"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00121}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "heroic police officer talks man down from edge of purchasing subway footlong sweet onion chicken teriyaki"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00056}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000123}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00128}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00013000000000000002}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00135}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "the political theology of trumpian evangelicalism"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.3e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00044000000000000007}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011200000000000001}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012000000000000001}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00011800000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012300000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "cousin really going all-in on retweeting porn stars"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000113}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0012}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "presidential candidates react to paris attacks"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00043000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011000000000000002}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00118}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "police assume truck was deliberately driven into berlin christmas market"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000114}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "society tea party spoiled by ocelot"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.4000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00045}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00011}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000119}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "independent-film festival crushed by paramount troops"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.4000000000000006e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00045}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00011}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000119}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00124}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "what is a reverse mortgage?"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.1e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00042}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011000000000000002}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00118}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012100000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "parents reminisce to children about dating algorithm that brought them together"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "where have all the children gone?"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00043000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011100000000000001}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00119}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "this 'trapped' clip is a snapshot of america's thorny abortion laws"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.1000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00052}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011999999999999999}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000126}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013100000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "omg, the duchess of cambridge is vogue uk's june cover star"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.4000000000000005e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00055}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0013100000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000129}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00134}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "selena gomez hits the beach in a bikini"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00046}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000114}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "quiet guy mistaken for nice guy"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.1e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00042}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00010800000000000001}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00118}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012100000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "little league pitcher totally in awe after giving up grand slam"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000113}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0012}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "are brick and mortar banks and checking accounts dying due to digital wallets, prepaid debit cards, etc.?"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00056}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0013200000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00013000000000000002}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00135}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "the challenge of exclusivity"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00041}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00010899999999999999}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000115}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012000000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "china sets stage for xi jinping to stay in office indefinitely"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "'finding dory' just keeps swimming past the box office competition"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "report: 79% of minority suspects receive miranda rights while unconscious"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "want to prevent lone wolf terrorism? promote a 'sense of belonging' among immigrants"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.1000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00052}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011999999999999999}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000126}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013100000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "yosemite expands lodging accommodations with new log cabin high-rises"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "a week in brooklyn, new york, on a $55,000 salary"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.1000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00052}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011999999999999999}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000126}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013100000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "federal judge rules fair housing law protects colorado lgbt couple"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "jason momoa clearly came to oscars straight from work"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00121}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "america's favorite mexican food chain is... not chipotle. not even close."}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.300000000000001e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00054}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000119}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00126}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000128}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013300000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "controversial christian faction believes jesus was nailed to two parallel pieces of wood"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.1000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00052}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000126}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013100000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "box with cooking instructions immediately retrieved from trash"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.3e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00044000000000000007}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011200000000000001}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00011800000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012300000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "former priest convicted in decades-old beauty queen slaying"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00046}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000114}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "new study finds solving every single personal problem reduces anxiety"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00046}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000114}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00119}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00125}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "discarded banana peel results in tragicomic tableau"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.4000000000000006e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00045}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00011}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000119}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00124}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "new mountain dew vows to kill 99.9% of stomach bacteria"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "blindfolded panetta shipped to kabul in hilarious cia hazing ritual"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.1000000000000006e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00052}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000118}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00125}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000126}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013100000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "nancy pelosi suggests donald trump get his mental health checked"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "mike pence condemns atheists, homosexuals, and feminists for role in forcing god to punish america on 9/11"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 6e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00061}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00012600000000000003}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00133}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000135}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0014}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "the fight to bring transparency to california's charter schools"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00046}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000114}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00125}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "theater: bradley cooper gets ugly; tr knight gets closeted"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.0005099999999999999}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000125}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013000000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "worker who forgot email attachment expects coworkers to forgive her just like that"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000115}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "study finds health benefits associated with seriously considering going vegetarian for a while now"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000118}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "bolton: 'we will not be drawn into a lengthy ground war in syria\u2014although, saying it out loud, that sounds incredible'"}], "ideal": "1", "gpt-3.5-turbo": {"response": "1", "cost": 6.4e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00065}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000133}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0013800000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00013900000000000002}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00144}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "world wonders what trump has on united states that's forcing nation to keep him in power"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00053}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000119}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00126}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000127}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "naked man mingles freely in locker room"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.4000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00045}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00011}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012000000000000001}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000119}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "gina rodriguez responds to golden globes' america ferrera mix-up"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00053}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00012100000000000001}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012900000000000001}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000127}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "can't go wrong with a cheeseburger, area man reports"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "women in business q&a: paula kavolius, founder and president, house of possibilities"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00056}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0013200000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00013000000000000002}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00135}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "turns out pope francis is a fan of beauty vloggers"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "cia to shift focus to greeting cards"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00043000000000000004}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011200000000000001}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "psa on funny or die shows the absurdity of valuing all opinions equally"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.1000000000000006e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00052}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000118}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00125}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000126}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013100000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "report: election day most americans' only time in 2016 being in same room with person supporting other candidate"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.8e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00059}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000125}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00132}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000133}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00138}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "don't bother asking this amazon echo anything"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.3e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00044000000000000007}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00011}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00011800000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012300000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "michelle obama: my proudest achievement as first lady is my daughters"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00053}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011999999999999999}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000127}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "squirrel who really chunked out unable to look neighborhood residents in eye"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000115}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "voyager probe badly damaged after smashing into end of universe"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000113}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0012}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "taylor swift's cover of an earth, wind & fire classic is pissing people off"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.300000000000001e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00054}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00012}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00127}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000128}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0013300000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "state department has spent none of the $120 million allocated to fight russian interference"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.0005099999999999999}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011899999999999999}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000125}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013000000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "nation demands nasa stop holding press conferences until they discover some little alien guys"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005099999999999999}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000125}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013000000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "is this a quote from donald trump or dialogue from porn?"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000115}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "nation's debate viewers disgusted with selves after connecting with mitt romney"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.8e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00049}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000115}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000123}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00128}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "area plant proudly displays leaf"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00041}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000107}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000115}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012000000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "eric trump: my dad isn't racist because he only 'sees 1 color, green'"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00056}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000123}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0013}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00013000000000000002}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00135}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "mysterious american flags in northern syria were planted by u.s. troops, pentagon says"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.4000000000000005e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00055}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00012}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0013000000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000129}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00134}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "pope francis condemns growing healthcare inequality in wealthy countries"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.7e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00047999999999999996}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000115}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.00012200000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00127}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "moral compass lost in woods"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.1e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00042}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00010899999999999999}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00114}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012100000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "trump voter fraud commissioner says panel should be more transparent or disband"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000117}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00122}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "new hampshire returns to obscurity"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.3e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00044000000000000007}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011200000000000001}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00117}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00011800000000000001}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012300000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "16 universally gross things no one really talks about"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.4000000000000006e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00045}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000114}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0012200000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000119}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00124}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "comparing donald trump to lord voldemort is unspeakably stupid. it's also pretty dangerous."}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.6e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00057}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.00012200000000000001}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0012900000000000001}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000131}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013599999999999999}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "michael bloomberg's 2016 ambitions may shake up the race -- and his media company"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.4000000000000005e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00055}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00012200000000000001}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0013000000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000129}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00134}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "you can has cheezburger: a guide to potential cheeses and what toppings to pair them with"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5.4000000000000005e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00055}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_gpt-4-1106-preview": {"response": "0", "cost": 0.0013100000000000002}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000129}, "instructor_base_gpt-4-1106-preview": {"response": "0", "cost": 0.00134}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "pentagon report concludes too many soldiers have same nickname"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00046}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000112}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00119}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.00012}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00125}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "someone replaced train sounds with screams, and it's genius"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 4.6e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00047000000000000004}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000113}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0012}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000121}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.00126}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "dalai lama swears he recognizes guy at party from past life"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 4.9e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000124}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0012900000000000001}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "scooby-doo movie says being a size 8 is a curse"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005099999999999999}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000116}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00123}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000125}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013000000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "hey, female directors, steven spielberg has some good news for you"}], "ideal": "0", "gpt-3.5-turbo": {"response": "0", "cost": 5e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.0005099999999999999}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.00011899999999999999}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00124}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000125}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013000000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "court rules meryl streep unable to be tried by jury as she has no peers"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.2000000000000004e-05}, "gpt-4-1106-preview": {"response": "1", "cost": 0.00053}, "instructor_gpt-3.5-turbo": {"response": "1", "cost": 0.000119}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.00126}, "instructor_base_gpt-3.5-turbo": {"response": "1", "cost": 0.000127}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013200000000000002}}
{"input": [{"role": "system", "content": "Respond with only a 1 or 0 to signify if the user's message includes sarcasm, or not"}, {"role": "user", "content": "stormy daniels '60 minutes' interview leads to spike in pornhub searches for anderson cooper"}], "ideal": "1", "gpt-3.5-turbo": {"response": "0", "cost": 5.6e-05}, "gpt-4-1106-preview": {"response": "0", "cost": 0.00057}, "instructor_gpt-3.5-turbo": {"response": "0", "cost": 0.000125}, "instructor_gpt-4-1106-preview": {"response": "1", "cost": 0.0013}, "instructor_base_gpt-3.5-turbo": {"response": "0", "cost": 0.000131}, "instructor_base_gpt-4-1106-preview": {"response": "1", "cost": 0.0013599999999999999}}
